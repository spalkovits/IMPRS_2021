{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "<center><img src=\"images/RWTH.jpg\" width=600></center>\n",
    "\n",
    "<h1 align=\"center\"> Machine learning approaches for the prediction of oxygen evolution catalysts. </h1> \n",
    "<h1 align=\"center\"> Dr. Stefan Palkovits </h1>\n",
    "<h1 align=\"center\"> <a href=\"stefan.palkovits@itmc.rwth-aachen.de\">stefan.palkovits@itmc.rwth-aachen.de </a> </h1>\n",
    "<h1 align=\"center\"> <a href=\"https://twitter.com/palkovitslab?lang=de\">@PalkovitsLab</a> </h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work we will use several Machine Learning algorithms to predict the overpotential of water oxidation catalysts. The models will be evaluated against each other.\n",
    "\n",
    "Two datasets will be used:\n",
    "\n",
    "* J. A. Haber, Y. Cai, S. Jung, C. Xiang, S. Mitrovic, J. Jin, A. T. Bell, J. M. Gregoire, Energy Environ. Sci., 2014, 7, 682–688, DOI: 10.1039/c3ee43683g\n",
    "\n",
    "* J. A. Haber, C. Xiang, D. Guevarra, S. Jung, J. Jin, J. M. Gregoire, ChemElectroChem 2014, 1, 524 – 528, DOI: 10.1002/celc.201300229\n",
    "\n",
    "The data from the supplementory information is already compiled for your convenience in a single csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization and preprocessing\n",
    "\n",
    "First we have to load the data with the help of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NiFeCoCe_full = pd.read_csv('./data/full_dataset.csv')\n",
    "NiFeCoCe_own = pd.read_csv('./data/own_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we take a look at it with the *head* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NiFeCoCe_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the data is just the elemental composition and the measured respective overpotential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization of the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will mix in some more sophistic visualization tools. Apart from the 'standard' Matplotlib we will use Seaborn for more modern graphics and moreover we will use Altair to have some additional features. Altair differs from Matplotlib and Seaborn as it wants you to specify the graphs in a declarative way to lead the focus on the actual visualization and not on the code for it.\n",
    "\n",
    "First it is often useful remove the limit in the entries that Altair can deal with. Maybe this is not necessary in later versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='https://altair-viz.github.io/', width=900, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable('default', max_rows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see in the next cell Altair works pretty different. We tell Altair\n",
    "\n",
    "* that we want a chart made from which dataframe,\n",
    "* how the markers should look like\n",
    "* and what encodings to use for X and Y.\n",
    "\n",
    "What we get is a decent looking graph which we can directly save if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(NiFeCoCe_full).mark_circle().encode(\n",
    "    alt.X('Ni', type='quantitative'),\n",
    "    alt.Y('Overpotential', type='quantitative'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want some kind of pairplot we have to extend this syntax. We add the rows and columns which we want to repeat and the size of the overall graph. And one power of Altair is the possible interactivity of the graphs which we also add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(NiFeCoCe_full).mark_circle().encode(\n",
    "    alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ").repeat(\n",
    "    row=['Overpotential'],\n",
    "    column=['Ni', 'Fe', 'Co', 'Ce']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clustering with k-means algorithm and further visualization with the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look for structures in the data we will go ahead now with some Unsupervised Learning. Although Principal Component Analysis is often this time it would fail (Try it!). So we will use a combination of\n",
    "\n",
    "* k-Means Clustering and\n",
    "* t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "for Unsupervised Learning and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding', width=900, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets first import the k-MEans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we do an exploratory elbow curve by initiating a set of k-Means instances in a for loop and fitting it to the dataset. Then we calculated the score to evaluate the performance of each clustering with respect to its size. The resulting plot can be found a little further down together with the results from t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_ellbow = []\n",
    "\n",
    "for E in range(20):\n",
    "    E = E+1\n",
    "    kmeans = KMeans(n_clusters=E)\n",
    "    kmeans.fit(NiFeCoCe_full.loc[:,'Ni':'Ce'])\n",
    "    score = kmeans.score(NiFeCoCe_full.loc[:,'Ni':'Ce'])\n",
    "    k_ellbow.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we do a final clustering and predict with entry in the dataset belongs to which cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cluster = KMeans(n_clusters=10)\n",
    "k_cluster.fit(NiFeCoCe_full.loc[:,'Ni':'Ce'])\n",
    "y_k_cluster = k_cluster.predict(NiFeCoCe_full.loc[:,'Ni':'Ce'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we import the t-SNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we initiate one instance of the algorithm with the respective hyperparameters. The *n_components* is easy to guess as we want a projection in a xy-plane but especially the perplexity is sometimes hard to figure out in this case 30 works decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=1000, init='pca')\n",
    "tsne_result = tsne.fit_transform(NiFeCoCe_full.loc[:,'Ni':'Ce'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot everything together. From the elbow plot on the left one sees that 10 seems to be a decent cluster amount. On the right hand side it seems that t-SNE was able to spread the data nice into the plane and using the cluster number for the coloring shows that the 10 cluster do not mix much. Apart from Seaborn here a *GridSpec* from Matplotlib is used to get a nice looking distribution of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster')\n",
    "plt.figure(figsize=(15,9))\n",
    "grid = plt.GridSpec(1,3)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "plt.xlim(0, 20)\n",
    "plt.title('K-means cluster number')\n",
    "plt.xlabel('n Cluster')\n",
    "plt.ylabel('grad Score')\n",
    "plt.plot(np.gradient(k_ellbow), 'k')\n",
    "\n",
    "plt.subplot(grid[0,1:])\n",
    "plt.xlim((-80,80))\n",
    "plt.ylim((-80,80))\n",
    "plt.title('t-distributed Stochastic Neighbor Embedding')\n",
    "plt.xlabel('x tSNE')\n",
    "plt.ylabel('y tSNE')\n",
    "plt.scatter(tsne_result[:,0], tsne_result[:,1], c=y_k_cluster, marker='o',alpha=0.5, cmap=plt.cm.get_cmap('magma', 10))\n",
    "plt.colorbar(ticks=np.arange(0,12))\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not mess up the dataset during the next visualization we now make a copy of the dataset and add the cluster numbers to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_Visualization = NiFeCoCe_full.copy()\n",
    "Cluster_Visualization['Cluster'] = pd.Series(y_k_cluster, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are in a browser with Jupyter we can to pretty nice visualizations with Altair. The next one is a variation of the pairplot from above. We added not only the clustering information on top of the pairplot as color but we are able to chose the cluster from a drop-down field and while hovering over a datapoint we can see the actual composition as tooltip. This helps us to get the information from the Unsupervised LEarning back to a format that is easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dropdown = alt.binding_select(options=['0','1','2', '3', '4', '5', '6', '7', '8'])\n",
    "selection = alt.selection_single(fields=['Cluster'], bind=input_dropdown, name='Number ')\n",
    "color = alt.condition(selection,\n",
    "                    alt.Color('Cluster:N'),\n",
    "                    alt.value('lightgray'))\n",
    "\n",
    "chart = alt.Chart(Cluster_Visualization).mark_circle().encode(\n",
    "    x='Ni',\n",
    "    y='Overpotential',\n",
    "    color=color,\n",
    "    tooltip=['Ni', 'Fe', 'Co', 'Ce']\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ").add_selection(\n",
    "    selection\n",
    ")\n",
    "\n",
    "chart | chart.encode(x='Co') | chart.encode(x='Fe') | chart.encode(x='Ce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sorting, splitting by overpotential and preprocessing for the Machine Learning algorithms\n",
    "\n",
    "Before we can go on with Supervised Learning we have to take some care about the splitting of the dataset. When we want to predict the overpotential then it is useful to know that it is desirable for a catalyst to produce an overpotential that is as low as possible. But there are only very few samples with a low overpotential in the dataset. To ensure that the samples with a low overpotential are available in both, the training and the test dataset, we first have to split the dataset and cut the low overpotentials in a separate set. Now it is sure that we have less than 150 samples with a low overpotential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NiFeCoCe_new = NiFeCoCe_full.sort_values(by=['Overpotential'])\n",
    "\n",
    "NiFeCoCe_small_OP = NiFeCoCe_new[NiFeCoCe_new['Overpotential']<380]\n",
    "NiFeCoCe_large_OP = NiFeCoCe_new[NiFeCoCe_new['Overpotential']>=380]\n",
    "\n",
    "print('Samples with a low overpotential: ', NiFeCoCe_small_OP.shape)\n",
    "print('Samples with a high overpotential', NiFeCoCe_large_OP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a *train_test_split* of both datasets and add the training and test portions together again. Now we can be sure that the training and test set have the same fraction of low overpotential samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, y1_test = train_test_split(NiFeCoCe_large_OP, test_size=0.3, random_state=42)\n",
    "X2_train, y2_test = train_test_split(NiFeCoCe_small_OP, test_size=0.3, random_state=42)\n",
    "\n",
    "NiFeCoCe_train = np.vstack((X1_train, X2_train))\n",
    "NiFeCoCe_test = np.vstack((y1_test, y2_test))\n",
    "\n",
    "print('Training samples: ', NiFeCoCe_train.shape)\n",
    "print('Test samples: ', NiFeCoCe_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we to a final shuffling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "order1 = np.argsort(np.random.random(NiFeCoCe_train[:,-1].shape))\n",
    "order2 = np.argsort(np.random.random(NiFeCoCe_test[:,-1].shape))\n",
    "\n",
    "NiFeCoCe_train = NiFeCoCe_train[order1]\n",
    "NiFeCoCe_test = NiFeCoCe_test[order2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we cut the features from the targets and divide the targets by 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NiFeCoCe_train[:,0:4]\n",
    "train_label = NiFeCoCe_train[:,-1]/1000\n",
    "\n",
    "test_data = NiFeCoCe_test[:,0:4]\n",
    "test_label = NiFeCoCe_test[:,-1]/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Artificial Neural Network Approach\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "* Neural Networks are state-of-the-art Machine Learning algorithms\n",
    "* Features mark the inputs\n",
    "* Targets are the outputs\n",
    "* The neurons are connected by weights\n",
    "* Each neuron has an additional activation function\n",
    "\n",
    "\n",
    "</font>\n",
    "    \n",
    "<center><img src=\"images/network2.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('Tensorflow version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_list = np.arange(2, 32, 2)\n",
    "neuron_error = []\n",
    "\n",
    "for neuron in neuron_list:\n",
    "        \n",
    "    model_opt = keras.Sequential([\n",
    "        layers.Dense(neuron, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "        layers.Dense(neuron, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "        ])\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    \n",
    "    model_opt.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "        \n",
    "    model_opt.fit(train_data, train_label, epochs=500, validation_split=0.2, verbose=0, callbacks=[early_stop])\n",
    "    \n",
    "    epochs = model_opt.history.epoch[-1]\n",
    "    \n",
    "    predictions_ann = model_opt.predict(test_data).flatten()\n",
    "    \n",
    "    error_ann = np.sqrt(mean_squared_error(test_label,predictions_ann))\n",
    "        \n",
    "    neuron_error.append(error_ann)\n",
    "    \n",
    "    print('\\r', 'Neurons used =', neuron, 'Error =', error_ann, 'Epochs =', epochs, end='')\n",
    "\n",
    "print(' Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.xlabel('Neurons used')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.plot(neuron_list, neuron_error);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.Dense(6, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "    \n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    \n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "model.fit(train_data, train_label, epochs=500, validation_split=0.2, verbose=0, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster', palette='magma')\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(model.history.epoch, np.array(model.history.history['loss']), 'k',label='Train set')\n",
    "plt.plot(model.history.epoch, np.array(model.history.history['val_loss']), 'm', label='Validation set')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.plot(model.history.epoch, np.array(model.history.history['mae']), 'k', label='Train set')\n",
    "plt.plot(model.history.epoch, np.array(model.history.history['val_mae']), 'm', label='Validation set')\n",
    "plt.legend()\n",
    "         \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_data).flatten()\n",
    "test_predictions = model.predict(test_data).flatten()\n",
    "\n",
    "predict_own_data = model.predict(NiFeCoCe_own.loc[:,'Ni':'Ce']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.title('Artificial Neural Network', pad=15)\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(test_label*1000, test_predictions*1000, marker='.', alpha=0.5)\n",
    "plt.plot([-1000, 1000], [-1000, 1000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Support vector regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {'gamma':[1, 10, 100],\n",
    "                'C':[1, 10, 100],\n",
    "                'epsilon':[0.001, 0.01, 0.1]}\n",
    "\n",
    "svr_tune = SVR(kernel='rbf')\n",
    "g_search = RandomizedSearchCV(svr_tune, hyper_params, cv=5, n_jobs=-1)\n",
    "g_search.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score', g_search.best_score_)\n",
    "print('Best Estimator ', g_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = g_search.best_estimator_.C\n",
    "best_gamma = g_search.best_estimator_.gamma\n",
    "best_epsilon = g_search.best_estimator_.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=10, gamma=10, epsilon=0.001)\n",
    "y_svr = svr_rbf.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svr_train = y_svr.predict(train_data)\n",
    "predict_svr = y_svr.predict(test_data)\n",
    "\n",
    "predict_own_svr = y_svr.predict(NiFeCoCe_own.loc[:,'Ni':'Ce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.title('Support Vector Regression', pad=15)\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(test_label*1000, predict_svr*1000, marker='.', alpha=0.5)\n",
    "plt.plot([-1000, 1000], [-1000, 1000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# k-Nearest Neighbour regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_val = []\n",
    "for K in range(50):\n",
    "    K = K+1\n",
    "    model_knn = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "    model_knn.fit(train_data, train_label)\n",
    "    predict_knn = model_knn.predict(test_data)\n",
    "    error = np.sqrt(mean_squared_error(test_label,predict_knn))\n",
    "    rmse_val.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.title('Validation error')\n",
    "plt.plot(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(rmse_val))\n",
    "best_k = np.argmin(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = neighbors.KNeighborsRegressor(n_neighbors = 11)\n",
    "model_knn.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_knn_train=model_knn.predict(train_data)\n",
    "predict_knn=model_knn.predict(test_data)\n",
    "\n",
    "predict_own_knn=model_knn.predict(NiFeCoCe_own.loc[:,'Ni':'Ce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.title('K-Nearest Neighbors', pad=15)\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(test_label*1000, predict_knn*1000, marker='.', alpha=0.5)\n",
    "plt.plot([-1000, 1000], [-1000, 1000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prediction overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster', palette='Greys_r')\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.title('Artificial Neural Network', pad=15)\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(test_label*1000, test_predictions*1000, marker='.', alpha=0.5)\n",
    "plt.plot([-1000, 1000], [-1000, 1000])\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.title('Support Vector Regression', pad=15)\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(test_label*1000, predict_svr*1000, marker='.', alpha=0.5)\n",
    "plt.plot([-1000, 1000], [-1000, 1000])\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.title('K-Nearest Neighbors', pad=15)\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(test_label*1000, predict_knn*1000, marker='.', alpha=0.5)\n",
    "plt.plot([-1000, 1000], [-1000, 1000])\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(NiFeCoCe_own.loc[:,'Overpotential'], predict_own_data*1000, marker='.', alpha=0.5)\n",
    "plt.annotate('outside dataset', xy=(305,428), size=15)\n",
    "plt.plot([-1000, 1000], [-1000, 1000])\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(NiFeCoCe_own.loc[:,'Overpotential'], predict_own_svr*1000, marker='.', alpha=0.5)\n",
    "plt.annotate('outside dataset', xy=(305,428), size=15)\n",
    "plt.plot([-1000, 1000], [-1000, 1000])\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.xlim((300,500))\n",
    "plt.ylim((300,500))\n",
    "plt.xlabel('True Values [mV]')\n",
    "plt.ylabel('Predictions [mV]')\n",
    "plt.scatter(NiFeCoCe_own.loc[:,'Overpotential'], predict_own_knn*1000, marker='.', alpha=0.5)\n",
    "plt.annotate('outside dataset', xy=(305,428), size=15)\n",
    "plt.plot([-1000, 1000], [-1000, 1000])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Calculating overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "mse_ann = mean_squared_error(test_label,test_predictions)\n",
    "mse_ann_train = mean_squared_error(train_label,train_predictions)\n",
    "\n",
    "mse_svr = mean_squared_error(test_label,predict_svr)\n",
    "mse_svr_train = mean_squared_error(train_label,predict_svr_train)\n",
    "\n",
    "mse_knn = mean_squared_error(test_label,predict_knn)\n",
    "mse_knn_train = mean_squared_error(train_label,predict_knn_train)\n",
    "\n",
    "r2_ann_train = r2_score(train_label,train_predictions)\n",
    "r2_ann = r2_score(test_label,test_predictions)\n",
    "\n",
    "r2_svr_train = r2_score(train_label,predict_svr_train)\n",
    "r2_svr = r2_score(test_label,predict_svr)\n",
    "\n",
    "r2_knn_train = r2_score(train_label,predict_knn_train)\n",
    "r2_knn = r2_score(test_label,predict_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(style='ticks', context='poster', palette='magma')\n",
    "\n",
    "ann = mpatches.Patch(color='k', label='Artificial Neural Network')\n",
    "svr = mpatches.Patch(color='m', label='Support Vector Regression')\n",
    "knn = mpatches.Patch(color='c', label='K-Nearest Neighbors')\n",
    "\n",
    "names = ['train', 'test', 'train', 'test', 'train', 'test']\n",
    "pos = range(len(names))\n",
    "colors = ['k', 'k', 'm', 'm', 'c', 'c']\n",
    "rotation = 0\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.suptitle('Overall metrics', y=1.02)\n",
    "plt.subplot(121)\n",
    "plt.bar(pos, [mse_ann_train, mse_ann, mse_svr_train, mse_svr,mse_knn_train, mse_knn], color=colors)\n",
    "plt.xticks(pos, names,rotation=rotation)\n",
    "plt.ylabel('mean squared error')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.bar(pos, [r2_ann_train,r2_ann, r2_svr_train, r2_svr,r2_knn_train, r2_knn], color=colors)\n",
    "plt.xticks(pos, names, rotation=rotation)\n",
    "plt.ylabel('R2 score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper center', handles=[ann, svr, knn], ncol=3, bbox_to_anchor=(-0.1, -0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "* Machine Learning can be used to predict the overpotential of literature datasets and catalysts from our own work\n",
    "\n",
    "* Simple models like Support Vector Regression are able to outperform more complex Artificial Neural Networks\n",
    "\n",
    "* Visualization, preprocessing of data and tuning of hyperparameters are important tasks for Machine Learning\n",
    "    \n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
